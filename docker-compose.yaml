version: "3"

services:

  spark-master:
    image: bde2020/spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./tweetsream:/spark/tweetstream
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - "constraint:node==spark-master"

  spark-worker-1:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    volumes:
      - ./tweetsream:/spark/tweetstream

    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "constraint:node==spark-worker1"

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOOKEEPER_CLIENT_PORT=2181

  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - '9092:9092'
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    depends_on:
      - zookeeper

  postgres:
      image: postgres:9.6
      environment:
          - POSTGRES_USER=airflow
          - POSTGRES_PASSWORD=airflow
          - POSTGRES_DB=airflow
      logging:
          options:
              max-size: 10m
              max-file: "3"

  webserver:
      image: airflow-tweetstream
      restart: always
      user: root
      depends_on:
          - postgres
      environment:
          - LOAD_EX=n
          - EXECUTOR=Local
      logging:
          options:
              max-size: 10m
              max-file: "3"
      volumes:
          - ${PWD}/orchestration/dags:/usr/local/airflow/dags
          - ./tweetstream:/usr/local/airflow/dags/tweetstream
          - ${PWD}/libs:/usr/local/airflow/dags/tweetstream/libs
      ports:
          - "5050:8080"
      command: webserver
      healthcheck:
          test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
          interval: 30s
          timeout: 30s
          retries: 3

  hadoop:
    image: sequenceiq/hadoop-docker:2.7.0
    ports:
      - "50010:50010"
      - "50020:50020"
      - "50070:50070"
      - "50075:50075"
      - "50090:50090"
      - "8020:8020"
      - "9000:9000"
      - "8088:8088" # Yarn